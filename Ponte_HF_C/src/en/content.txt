Abstract Measuring and providing efficiency of educational applications is a serious, open problem, which impacts the future of this expanding industry greatly. Reaching player engagement is a complex challenge, as it also depends on the given task and the mental state of the player. Researches answer this by using adaptive educational games. To reach the goal, however, knowledge about more parameters is required about the game tasks, the abilities of the player, his actual physiological state and performance as well. In this paper we present our results, which use a biofeedback based adaptive algorithm, and based on this, an innovative psychometric model to take a step towards maximizing user engagement. Keywords biofeedback, cognitive abilities, item response theory, multiple intelligence, classification
People like doing something if they are enjoying it. Many activities, especially playing games can reach a point when the subjects are engaged and want to perform the activity continuously. Nowadays, in video games, things like visual effects, the reward system or the instant feedback over the performance can achieve this engagement easily. Players can learn moves or methods during the gameplay. This could make the games ideal candidates for educational purposes, if we can achieve the same easy learning effect. These games, that have educational purposes beside the entertainment, are called serious games. Motivating the users during serious games is a key problem . If the motivation is high, and the user enjoys the game, it can lead to longer and more effective playing sessions. For the most suitable gameplay, beside the ergonomic aspects, the appropriate level of difficulty is extremely important. It affects the mental state of the user, which, in turn, affects the performance. The theory of Flow describes this approach. Typically, at least three relevant mental states are identified, based on Csíkszentmihályi's theory: boredom, frustration and engagement. The user is in the state of boredom if the level of the task is lower than the skill of the user. Frustration is defined as a state, where the task demands are higher than the skill of the user. Engagement is the ideal state, where the level of the task and the skill of the user meet. This state leads to the Flow experience, where the efficiency is maximized. Many studies aim to introduce adaptive difficulty management in serious games . The adaptiveness of these systems are based on the performance of the user in most cases, but for the ideal learning efficiency, the mental state of the user should also be considered. To create an adaptive experience, adjusting the game difficulty based on the abilities and mental state of the player is found to be of key importance. In many games, gameplay can be divided into several well-defined tasks (steps or milestones). Each of these tasks has a difficulty level from which the difficulty of gameplay is determined. By giving the appropriate tasks for each player, their performance can be maximized, which enables more efficient learning and provides motivation. In case the abilities of players can be measured, mathematical models can be used to estimate the probability that a player solves a given game task correctly. Item Response Theory (IRT) is particularly useful for this purpose [5-9]. In an educational game, each game task corresponds to a test item and each player corresponds to a subject in an IRT model. In some cases, no estimate is available for the difficulty of game tasks. Fortunately, IRT models, being mathematical, can be estimated in a lot of ways. Using model estimation, we can approximately measure the parameters of IRT items, which means, the difficulty of game tasks can be estimated using data from the responses of players. By our theory, item parameter estimation can be simplified if we determine the ability of players via alternate means. One such method is by measuring a person's intelligence, based on the Multiple Intelligence Theory of H. Gardner. In his theory, Gardner states that intelligence can be differentiated into many inter-correlated modalities, rather than one single overlaying construct [19]. These modalities are defined as 9 individually describable and measurable intelligence fields. In this paper, we present an automated process to measure a number of the areas. We use the result of these automated measurement as the input of the IRT calculations. We also provide a simplified method for estimating the difficulty of game tasks when the ability of players is treated as known. Our approach uses a modified version of IRT model estimation. Furthermore, we also propose an extension of simple IRT models, which can take the mental effort of subjects into account when determining the probability of solving a specific task by a specific user. Based on that information, we can manipulate the gameplay in order to keep the player in the state of Flow. The rest of this paper provides a deeper insight into our system. The next section describes the related work and the theories, which our solution is based on. In the following chapter, the classification-based mental effort measurements are presented. After this, we show how cognitive games can be used to measure the ability of someone. The following chapters of this section describe the IRT model which uses mental effort and ability calculated by these cognitive games as parameters. A summary of our results and future works conclude our paper.
The mental effort or cognitive workload of the user could be concluded from physiological parameters. In our research, we measure mental effort through two physiological value: the hearth rate variability and the EEG data.  Heart rate is influenced by the activity of both the sympathetic and parasympathetic branch of the autonomic nervous system. An increase in sympathetic activity raises the hearth rate, while an increase in parasympathetic activity lowers it. High mental effort levels cause increased heart rate, and decreased heart rate variability. Both time- and frequency domain-based analysis can be used to infer the mental effort of the user. A parametric auto regression (AR) based power spectral density estimation and a moving average (MA) based time domain approach are primary used in the analysis. We use the Low frequency (0.07-0.15 Hz) component as a measure of the mental effort . Compared to a baseline value, a lower ratio would mean parasympathetic, while a higher would signal sympathetic dominance. The aim of the MA method is to catch any changes in frequency (RR intervals). EEG measures brain activity based on electric changes elicited by neurons. These, 10-100 microVolt signals can be measured with electrodes on the scalp . In the last years, EEG became very common in human-computer interaction There are many metrics to measure the mental workload based on EEG. Most studies use the power estimates of certain frequency bands (alpha, beta, delta or theta). Higher levels of load are accompanied by lower overall alpha power over activated cortical areas. Item Response Theory (IRT) is a method for designing and scoring tests that consist of simple questions, which are called test items. The term item is generic: it can be used for a variety of question types, including multiple choice and open-ended questions. However, for an item, it should be easily determinable whether a given subject answered correctly. IRT can be used to measure the ability of subjects taking the test. Ability is a latent trait, which means it is not a directly measurable characteristic of subjects. According to IRT, the test performance of subjects can be predicted or explained based on their ability level. Thus, ability is defined as a parameter of subjects that correlates with their performance on the test. Generally speaking, higher ability means higher probability that the subject gives a correct answer to the test item. IRT defines mathematical models which are called item response models. Unidimensional IRT models, which are the most commonly used ones in practice, consider a single trait for subjects, the aforementioned ability. Another way to categorize IRT models is whether their test items each have a single correct answer or not. In the first case, the items are called dichotomous. In the latter case, responses each have (a different) score value, meaning the items are polytomous. The common in all IRT models is that they define a function for each test item that is called the item response function (IRF). In unidimensional models, subject ability is the single input variable of this function. Most commonly, the IRF is a modified logistic function. In this case, the model is called a logistic model. An alternative formulation uses the cumulative distribution function (CDF) of the normal distribution. These models are sometimes called normal ogive models. Nevertheless, in each case, the plot of the IRF (the item characteristic curve, ICC) is of a sigmoid shape. IRT models differ in that how many factors they consider when characterizing test items. The one parameter logistic model (1PL, sometimes also called the Rasch model) defines a single parameter for items: the item difficulty, which determines at which ability value the midpoint of the ICC (the mean of the value for the lower and the higher asymptotes) is located. The two parameter logistic model (2PL) adds discrimination as an item parameter, which characterizes the degree the item discriminates between subjects with different ability levels. Mathematically, discrimination is the slope for the tangent of the curve at its midpoint. 1PL assumes this degree is equivalent for all test items. Moreover, 1PL and 2PL do not consider that a subject can determine the correct answer for an item purely by guessing: in these models, guessing is part of the ability. The three parameter logistic model (3PL) takes guessing into account as the lower asymptote of the ICC.
When considering the traditional applications of IRT models in practice, the item and subject parameters are usually unknown at some stage of the model specification. When specifying the model, the item parameters need to be estimated. Typically, a random calibration sample from a target population is used for this purpose, where item and subject parameters are estimated simultaneously. This estimation can be done with a variety of methods. In each case, the item and subject parameters are unobservable, thus a problem of indeterminacy arises. This is formally called the identification problem. Simply speaking, this means that certain transformations leave the IRF invariant when estimating two and three parameter models. In these cases, either the subject abilities or the item difficulties are fixed so that their mean is 0 and their standard deviation is 1. One of the methods used for simultaneous parameter estimation is called joint maximum likelihood estimation (JMLE). Using JMLE, the values of item parameters and subject parameters need to be estimated in a way that they jointly maximize the value of the logarithm of the likelihood function. Variants of the iterative Newton-Raphson procedure or an alternative procedure, called Fisher's method of scoring can be used when performing this kind of estimation. Other methods include conditional maximum likelihood estimation (CMLE) that can be used when sufficient statistics are available for the subject parameters which is only true in case of the one parameter model. Marginal maximum likelihood estimation (MMLE)of the item parameters is carried out by integrating or summing over with respect to the subject parameters, and the function to be maximized is named the marginal likelihood function in this case. Two and three parameter models can also be estimated using MMLE. Finally, Bayesian methods can also be used for estimation. After the item parameters are estimated from the calibration sample, they are treated as known in subsequent applications and item banks are constructed. The task is then to estimate the ability of subjects taking a test made from the items of the item bank. Simple maximum likelihood estimation (MLE) is most commonly used for this purpose.  Maximum likelihood estimators have some particularly useful properties like asymptotic normality and consistency. The standard error for the ability estimate can also be obtained using MLE which enables approximate confidence intervals to be calculated. In case of a perfect or zero score, MLE fails. Bayesian methods can be considered as an alternative solution when this problem arises. Bayes estimators have smaller standard errors but require the specification of a prior belief regarding the ability of subjects. In his multiple intelligence theory, Gardner states that intelligence can be differentiated into many inter-correlated modalities, rather than one single overlaying construct. He specifies nine fields of intelligence: musical-rhythmic, visual-spatial, verbal-linguistic, logical-mathematical, bodily-kinesthetic, interpersonal, intrapersonal, naturalistic and existential. There are existing methods to measure cognitive abilities. One example is the Map of Interest Method, created by Eva Gyarmathy]. Based on the same Multiple Intelligence theory of Gardner, it uses questionnaires and self-evaluation to measure abilities. The subjects have to classify how much they agree with a given statement on a scale of 1 to 5, where the statements represent one of the 9 intelligence fields. The results can be used to determine the strengths and weaknesses of the individual where talent may manifest itself. There are several other measurement methods based on the Multiple Intelligence Theory. Profiling questionnaires are provided by other researches to measure multiple intelligence. In their research, Tirri, Nokelainen and Komulainen argue that sensitivities is a better word to call what Gardner calls intelligences. Their analysis concludes that "there is no definite answer to the basic question, whether the multiple intelligences model can be confirmed in self-evaluated intelligence". When comparing a learning strategy based on Gardners MI theory to a classic learning strategy, Mazaheri and Fatemis measurement shows that higher results can be achieved, if we take the students' preferred intelligence fields in account. They conclude, "that e-learning strategy based on multiple intelligences helps students gain a basic understanding of scientific concepts since has been offered the curriculum in a meaningful and personalized manner for students were not adaptable in compared with traditional learning methods. MI has also been integrated to the learning system, with promising results". According to Hafidi and Lamia's research, "Experimental results show that the proposed system can precisely provide personalized activity recommendations on-line based on learner abilities and responses, and moreover can accelerate learner learning efficiency and effectiveness." Learning Styles is another theory, which can be applied to enhance personalized learning.
